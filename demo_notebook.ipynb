{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWSome\n",
    "A clean wrapper over boto3 inspired by the user friendly aws cli.\n",
    "\n",
    "## Sessions\n",
    "All actions take an optional session parameter. If none is specified it will create one with whatever default configuration you have in ~/.aws/ directory.\n",
    "\n",
    "Since AWSome encourages writing your script once and running it in different environments, it isn't advisable to have a default session because there is the risk that you might run your script accidentally in your real S3. On the other hand it's not too clean or practical to give every action a session and you would have to remember to create new sessions and give your script the right ones.\n",
    "\n",
    "The recommended way is to write your script without worrying about sessions and use one of our context managers to provide it explicitly without changing your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awsome import s3\n",
    "# Default session\n",
    "buckets = s3.ls()\n",
    "\n",
    "# Explicitly defined session\n",
    "session = boto3.session.Session(\n",
    "        aws_access_key_id='XXXXXX',\n",
    "        aws_secret_access_key='XXXXXX',\n",
    "        region_name='eu-west-1'\n",
    "    )\n",
    "keys = s3.ls('s3://bucket/key', session=session)\n",
    "\n",
    "# Explicitly defined session with context manager\n",
    "from awsome.playground import boto3_session\n",
    "\n",
    "session = boto3.session.Session(\n",
    "        aws_access_key_id='XXXXXX',\n",
    "        aws_secret_access_key='XXXXXX',\n",
    "        region_name='eu-west-1'\n",
    "    )\n",
    "\n",
    "with boto3_session(session):\n",
    "    buckets = s3.ls()\n",
    "    \n",
    "# Profile defined in ~/.aws/credentials\n",
    "from awsome.playground import with_profile\n",
    "\n",
    "with aws_profile(profile='myprofile'):\n",
    "    buckets = s3.ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example usage of AWSome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will rename all the files from a bucket with a foo/bar/ prefix and copy them to another bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "from awsome import s3\n",
    "from awsome.playground import s3_sandbox, dry_run_sandbox, aws_profile, create_mock_keys\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intervention script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the function that will do the renaming/copy intervention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intervention():\n",
    "    keys = s3.ls('s3://testbucket/foo/bar/')\n",
    "    for key in keys:\n",
    "        new_key = key.replace('customers', 'clients')\n",
    "        # Rename all objects with the same key\n",
    "        s3.move_key(from_bucket='testbucket', from_key=key, to_bucket='testbucket', to_key=new_key)\n",
    "        # Move all objects to the production bucket with the same key\n",
    "        s3.copy_key(from_bucket='testbucket', from_key=new_key, to_bucket='prodbucket')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the script in production we should do a few tests to make sure it's doing what we think it is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create some dummy files that mimic the structure of the files in our real aws instance to test the intervention on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_data():\n",
    "    for i in range(1, 6):\n",
    "        # Files we will be changing\n",
    "        key = f'foo/bar/customers_{i}.csv'\n",
    "        s3.upload_string(data='some data', bucket='testbucket', key=key)\n",
    "        \n",
    "        # Files we want untouched\n",
    "        key = f'foo/baz/companies_{i}.csv'\n",
    "        s3.upload_string(data='some data', bucket='testbucket', key=key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that the test data is created correctly. Of course we don't want to actually create those dummy files in aws. \n",
    "\n",
    "Instead we use the s3_sandbox context manager that provides a moto s3 instance where we can run our tests as they would run in a real S3 instance. We just need to pass it the name of the buckets it needs to create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test bucket:\n",
      "aws s3 ls --recursive s3://testbucket/\n",
      "[   'foo/bar/customers_1.csv',\n",
      "    'foo/bar/customers_2.csv',\n",
      "    'foo/bar/customers_3.csv',\n",
      "    'foo/bar/customers_4.csv',\n",
      "    'foo/bar/customers_5.csv',\n",
      "    'foo/baz/companies_1.csv',\n",
      "    'foo/baz/companies_2.csv',\n",
      "    'foo/baz/companies_3.csv',\n",
      "    'foo/baz/companies_4.csv',\n",
      "    'foo/baz/companies_5.csv']\n",
      "\n",
      "Prod bucket:\n",
      "aws s3 ls --recursive s3://prodbucket/\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "with s3_sandbox(['testbucket', 'prodbucket']):\n",
    "    create_test_data()\n",
    "    print('Test bucket:')\n",
    "    pp.pprint(s3.ls('s3://testbucket/', recursive=True))\n",
    "    print('\\nProd bucket:')\n",
    "    pp.pprint(s3.ls('s3://prodbucket/', recursive=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the sample data has been created correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the intervention script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure that the script does what we want it to we will execute it with a dry run. This means that we won't actually execute the commands, just print the equivalent aws cli commands so we can visually inspect them.\n",
    "\n",
    "One exception is that we don't want to patch the ls function (it doesn't change S3 so it is reasonable not to patch it) because we depend on its output to generate the rest of the commands. We will need set patch_ls to false.\n",
    "\n",
    "The dry run sandbox context manager also creates a moto instance of S3 so we can rest assured that everything will execute in a sandbox and won't affect our real S3 instance. There is a variant of this context manager called dry_run that just patches the functions but doesn't create a sandbox, so don't use it unless you are sure that all the awsome commands in your code are patched by dry_run (in the future all of them should be patched) and you don't have any boto3/botocore calls (or just run it under a with s3_sandbox)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aws s3 ls s3://testbucket/foo/bar/\n",
      "aws s3 cp s3://testbucket/foo/bar/customers_4.csv s3://testbucket/foo/bar/clients_4.csv\n",
      "aws s3 rm s3://testbucket/foo/bar/customers_4.csv\n",
      "aws s3 cp s3://testbucket/foo/bar/clients_4.csv s3://prodbucket/foo/bar/clients_4.csv\n",
      "aws s3 cp s3://testbucket/foo/bar/customers_5.csv s3://testbucket/foo/bar/clients_5.csv\n",
      "aws s3 rm s3://testbucket/foo/bar/customers_5.csv\n",
      "aws s3 cp s3://testbucket/foo/bar/clients_5.csv s3://prodbucket/foo/bar/clients_5.csv\n",
      "aws s3 cp s3://testbucket/foo/bar/customers_2.csv s3://testbucket/foo/bar/clients_2.csv\n",
      "aws s3 rm s3://testbucket/foo/bar/customers_2.csv\n",
      "aws s3 cp s3://testbucket/foo/bar/clients_2.csv s3://prodbucket/foo/bar/clients_2.csv\n",
      "aws s3 cp s3://testbucket/foo/bar/customers_3.csv s3://testbucket/foo/bar/clients_3.csv\n",
      "aws s3 rm s3://testbucket/foo/bar/customers_3.csv\n",
      "aws s3 cp s3://testbucket/foo/bar/clients_3.csv s3://prodbucket/foo/bar/clients_3.csv\n",
      "aws s3 cp s3://testbucket/foo/bar/customers_1.csv s3://testbucket/foo/bar/clients_1.csv\n",
      "aws s3 rm s3://testbucket/foo/bar/customers_1.csv\n",
      "aws s3 cp s3://testbucket/foo/bar/clients_1.csv s3://prodbucket/foo/bar/clients_1.csv\n"
     ]
    }
   ],
   "source": [
    "with dry_run_sandbox(['testbucket', 'prodbucket'], patch_ls=False):\n",
    "    create_test_data()\n",
    "    intervention()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executing the intervention script in a sandbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where it gets interesting. We have inspected the dry run and everything looks reasonable, but you can never be too careful. To make sure we get it right we will execute the real script inside a moto S3 sandbox."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test bucket before:\n",
      "aws s3 ls --recursive s3://testbucket/\n",
      "[   'foo/bar/customers_1.csv',\n",
      "    'foo/bar/customers_2.csv',\n",
      "    'foo/bar/customers_3.csv',\n",
      "    'foo/bar/customers_4.csv',\n",
      "    'foo/bar/customers_5.csv',\n",
      "    'foo/baz/companies_1.csv',\n",
      "    'foo/baz/companies_2.csv',\n",
      "    'foo/baz/companies_3.csv',\n",
      "    'foo/baz/companies_4.csv',\n",
      "    'foo/baz/companies_5.csv']\n",
      "\n",
      "Prod bucket before:\n",
      "aws s3 ls --recursive s3://prodbucket/\n",
      "[]\n",
      "\n",
      "\n",
      "Starting intervention:\n",
      "aws s3 ls s3://testbucket/foo/bar/\n",
      "aws s3 cp s3://testbucket/foo/bar/customers_4.csv s3://testbucket/foo/bar/clients_4.csv\n",
      "aws s3 rm s3://testbucket/foo/bar/customers_4.csv\n",
      "aws s3 cp s3://testbucket/foo/bar/clients_4.csv s3://prodbucket/foo/bar/clients_4.csv\n",
      "aws s3 cp s3://testbucket/foo/bar/customers_5.csv s3://testbucket/foo/bar/clients_5.csv\n",
      "aws s3 rm s3://testbucket/foo/bar/customers_5.csv\n",
      "aws s3 cp s3://testbucket/foo/bar/clients_5.csv s3://prodbucket/foo/bar/clients_5.csv\n",
      "aws s3 cp s3://testbucket/foo/bar/customers_2.csv s3://testbucket/foo/bar/clients_2.csv\n",
      "aws s3 rm s3://testbucket/foo/bar/customers_2.csv\n",
      "aws s3 cp s3://testbucket/foo/bar/clients_2.csv s3://prodbucket/foo/bar/clients_2.csv\n",
      "aws s3 cp s3://testbucket/foo/bar/customers_3.csv s3://testbucket/foo/bar/clients_3.csv\n",
      "aws s3 rm s3://testbucket/foo/bar/customers_3.csv\n",
      "aws s3 cp s3://testbucket/foo/bar/clients_3.csv s3://prodbucket/foo/bar/clients_3.csv\n",
      "aws s3 cp s3://testbucket/foo/bar/customers_1.csv s3://testbucket/foo/bar/clients_1.csv\n",
      "aws s3 rm s3://testbucket/foo/bar/customers_1.csv\n",
      "aws s3 cp s3://testbucket/foo/bar/clients_1.csv s3://prodbucket/foo/bar/clients_1.csv\n",
      "Ending intervention:\n",
      "\n",
      "\n",
      "Test bucket after:\n",
      "aws s3 ls --recursive s3://testbucket/\n",
      "[   'foo/bar/clients_1.csv',\n",
      "    'foo/bar/clients_2.csv',\n",
      "    'foo/bar/clients_3.csv',\n",
      "    'foo/bar/clients_4.csv',\n",
      "    'foo/bar/clients_5.csv',\n",
      "    'foo/baz/companies_1.csv',\n",
      "    'foo/baz/companies_2.csv',\n",
      "    'foo/baz/companies_3.csv',\n",
      "    'foo/baz/companies_4.csv',\n",
      "    'foo/baz/companies_5.csv']\n",
      "\n",
      "Prod bucket after:\n",
      "aws s3 ls --recursive s3://prodbucket/\n",
      "[   'foo/bar/clients_1.csv',\n",
      "    'foo/bar/clients_2.csv',\n",
      "    'foo/bar/clients_3.csv',\n",
      "    'foo/bar/clients_4.csv',\n",
      "    'foo/bar/clients_5.csv']\n"
     ]
    }
   ],
   "source": [
    "with s3_sandbox(['testbucket', 'prodbucket']):\n",
    "    create_test_data()\n",
    "    print('Test bucket before:')\n",
    "    pp.pprint(s3.ls('s3://testbucket/', recursive=True))\n",
    "    print('\\nProd bucket before:')\n",
    "    pp.pprint(s3.ls('s3://prodbucket/', recursive=True))\n",
    "    \n",
    "    print('\\n\\nStarting intervention:')\n",
    "    intervention()\n",
    "    print('Ending intervention:')\n",
    "    \n",
    "    print('\\n\\nTest bucket after:')\n",
    "    pp.pprint(s3.ls('s3://testbucket/', recursive=True))\n",
    "    print('\\nProd bucket after:')\n",
    "    pp.pprint(s3.ls('s3://prodbucket/', recursive=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we have succesfully validated our script, and we can rest assured that it will do what we intend it to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating mock keys\n",
    "Say you want to replicate your keys from your real bucket into your sandbox. There is an easy way to do that.\n",
    "\n",
    "First we need to read the keys from your buckets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with aws_profile(profile='myprofile'):\n",
    "    test_keys = s3.ls('s3://test-bucket/', recursive=True)\n",
    "    prod_keys = s3.ls('s3://production-bucket/', recursive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can use the function create_mock_keys to populate those keys with random (short) data, therefore recreating the same keys that we have in our real buckets. This can help us be even more sure that the script runs correctly.\n",
    "\n",
    "One caveat is that create_mock_keys requires a marker bucket as a validation before it works so we will have to pass it create_marker_bucket=True. This is a safety measure because if you accidentally run this on your real S3 you likely (hopefully!) won't have a bucket with the same name as our marker_bucket and therefore it won't create the mock keys and won't overwrite your data. Still, exercise caution to only use it in a sandbox."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awsome.playground import s3_sandbox\n",
    "\n",
    "with s3_sandbox(buckets=['bi-analytics-hbg-test', 'bi-analytics-hbg'], create_marker_bucket=True):\n",
    "    create_mock_keys('test-bucket', test_keys)\n",
    "    create_mock_keys('production-bucket', prod_keys)\n",
    "    \n",
    "    intervention()\n",
    "    \n",
    "    pp.pprint(s3.ls('s3://test-bucket/.../', recursive=True))\n",
    "    print('\\n')\n",
    "    pp.pprint(s3.ls('s3://prod-bucket/.../', recursive=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
